{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6acdd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, BadRequestError\n",
    "from openai.types.chat import ChatCompletion\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "import time\n",
    "from typing import Optional\n",
    "import base64\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from openai import OpenAI\n",
    "import dataclasses\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d64d7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimumDelay:\n",
    "    def __init__(self, delay: float | int):\n",
    "        self.delay = delay\n",
    "        self.start = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        end = time.time()\n",
    "        seconds = end - self.start\n",
    "        if self.delay > seconds:\n",
    "            time.sleep(self.delay - seconds)\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=90), stop=stop_after_attempt(3))\n",
    "def chat(client: OpenAI, delay: float | int, **kwargs) -> ChatCompletion | None:\n",
    "    try:\n",
    "        with MinimumDelay(delay):\n",
    "            return client.chat.completions.create(**kwargs)\n",
    "    except BadRequestError as e:\n",
    "        print(f\"Bad Request: {e}\")\n",
    "        if \"safety\" in e.message:\n",
    "            return None\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        raise e\n",
    "def print_messages(messages):\n",
    "    for message in messages:\n",
    "        if isinstance(message[\"content\"], list):\n",
    "            print(f\"{message['role']}:\")\n",
    "            for content in message[\"content\"]:\n",
    "                if content[\"type\"] == \"text\":\n",
    "                    print(content[\"text\"])\n",
    "                elif content[\"type\"] == \"image_url\":\n",
    "                    print(\"[IMAGE]\")\n",
    "        else:\n",
    "            print(f\"{message['role']}: {message['content']}\")\n",
    "        print()\n",
    "    print(\"=========================================\")\n",
    "\n",
    "def read_jsonl(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                ex = json.loads(line)\n",
    "                yield ex\n",
    "\n",
    "def write_jsonl(path, data):\n",
    "    with open(path, \"w\") as f:\n",
    "        for ex in data:\n",
    "            f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4557eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class ChatCompletionConfig:\n",
    "    seed: int\n",
    "    delay: int\n",
    "    model: str\n",
    "    max_tokens: int\n",
    "    temperature: float\n",
    "    system_prompt: str\n",
    "    user_prompt: str\n",
    "    response_format: dict | None = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f96618db",
   "metadata": {},
   "outputs": [],
   "source": [
    "demos = [{\n",
    "    'filename': 'problem_frames/workplace_discrimination.jsonl',\n",
    "    'result': {\n",
    "        'relations': {\n",
    "            'paraphrases': [['f2737', 'f5998'], ['f5186', 'f5610'],['f297', 'f540']],\n",
    "            'contradictions': []\n",
    "        }\n",
    "    }\n",
    "}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b43d10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames=['disrespect_towards_women.jsonl','exploitation_of_women_by_men.jsonl','conspiracy_thinking.jsonl','professional_misconduct.jsonl','toxic_masculinity.jsonl','trivializing_the_need_for_representation.jsonl','homophobia.jsonl','promoting_self_harm.jsonl','trivializing_eating_disorders.jsonl','endorsing_marital_rape.jsonl'\n",
    "           'sexist_pseudoscience.jsonl','trivializing_addiction.jsonl','endorsing_necrophilia.jsonl','wage_disparity.jsonl','single_motherhood_stigmatization.jsonl','lack_of_accountability_by_men.jsonl','forced_marriage.jsonl','patriarchal_attitudes.jsonl','male_validation.jsonl','neo_sexism.jsonl','classism.jsonl',\n",
    "'gatekeeping.jsonl','commitment_phobia.jsonl','workplace_discrimination.jsonl',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ca09a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_prompt(jsonl_file_path: str, user_prompt: str,  demo: bool = False,  demo_result: dict | None = None):\n",
    "    result_lines = []\n",
    "    msg = []\n",
    "    \n",
    "    with open(jsonl_file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            frame_id = data['id']\n",
    "            frame_text = data['frame']\n",
    "            result_lines.append(f\"{frame_id}: {frame_text}\")\n",
    "    frame_lines= '\\n'.join(result_lines)\n",
    "    final=user_prompt.format(frames=frame_lines)\n",
    "    msg.append(\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": final},\n",
    "\n",
    "        ]}\n",
    "    )\n",
    "    if demo:\n",
    "        msg.append(\n",
    "            {\"role\": \"assistant\", \"content\": json.dumps(demo_result)}\n",
    "        )\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42a85855",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = 'prompts/relations.yaml'\n",
    "with open(config_file_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    config = ChatCompletionConfig(**config)\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'], timeout=90)\n",
    "sys_prompt = config.system_prompt.strip()\n",
    "user_prompt= config.user_prompt.strip()\n",
    "processed=set()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1474a214",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg=[{\"role\": \"system\", \"content\": sys_prompt}]\n",
    "for demo in demos:\n",
    "    msg.extend(make_user_prompt(demo['filename'],user_prompt, True, demo['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bcfc74d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': \"You are an expert linguistic assistant. Misogyny is defined as hatred of, aversion to, or prejudice against women.  Frames of Misogyny select particular aspects of misogyny and make them salient in communicating a message. Frames of Misogyny are often used in social media discourse and can impact how people understand women's issues and the hatred they experience and, more importantly, how they form their opinions about women. You will be given a list of frames, each with a unique ID. Your task is to identify relationships between these frames. We are interested in two specific types of relationships: 1. Paraphrases(X,Y): X and Y say essentially the same exact thing, with different words or phrasing. If one person agreed with X, they would agree with Y, and vice versa. Frames should share the same cause and the same problem to be considered paraphrases. 2. Contradicts(X,Y): X and Y contradict each other, such that they frame the same exact issue from opposing perspectives. If one person agreed with X, they would disagree with Y, and vice versa. The two frames X and Y should essentially paraphrase each other, sharing the same problem and cause but from opposing perspectives. Please follow this process: First, identify all paraphrase relationships among the frames. Then, examine these paraphrase clusters for possible contradictions. If no relationships exist for either types, return empty list. For every relationship you identify, use the frame IDs to specify the relationship in the list.\"},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'Frames of Misogyny:\\nf297: Women should be hired based on their appearance rather than their professional abilities.\\nf540: Women in professional roles are meant to be valued only for their physical appearance, not for their skills.\\nf1188: It is acceptable to deny women the promotions and raises they deserve.\\nf2737: Womenâ€™s professional roles are inherently less valuable and legitimate than those of men.\\nf3441: Women should be fired from their jobs and forced back into traditional domestic roles.\\nf5186: Women are not valued for their professional skills and contributions.\\nf5267: Womenâ€™s work is less valuable and therefore deserves lower pay.\\nf5610: Women are not meant to be desired or valued in the workplace.\\nf3911: Women should not be in professional kitchens.\\nf5998: Womenâ€™s jobs are inferior to menâ€™s jobs.'}]},\n",
       " {'role': 'assistant',\n",
       "  'content': '{\"relations\": {\"paraphrases\": [[\"f2737\", \"f5998\"], [\"f5186\", \"f5610\"], [\"f297\", \"f540\"]], \"contradictions\": []}}'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2446c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c1c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_INTERVAL = 3\n",
    "counter = 0\n",
    "\n",
    "def append_jsonl(filepath, data):\n",
    "    with open(filepath, 'a', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "def save_processed_set(filepath, processed_set):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(processed_set, f)\n",
    "\n",
    "# Initialize\n",
    "all_results = []\n",
    "\n",
    "for filename in sorted(os.listdir('problem_frames')):\n",
    "    if (filename in filenames) or (filename in processed):\n",
    "        continue\n",
    "    final_path=os.path.join('problem_frames', filename)\n",
    "\n",
    "    msg.extend(make_user_prompt(final_path, user_prompt))\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=config.model,\n",
    "            messages=msg,\n",
    "            temperature=config.temperature,\n",
    "            seed=config.seed,\n",
    "            response_format=config.response_format,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"API call failed for {filename}: {e}\")\n",
    "        msg.pop()\n",
    "        continue\n",
    "\n",
    "    msg.pop()\n",
    "\n",
    "    if not completion:\n",
    "        print(f\"Skipping meme {filename} due to API safety error\")\n",
    "    else:\n",
    "        content = completion.choices[0].message.content\n",
    "        print(f\"{filename} {completion.usage.prompt_tokens_details.cached_tokens}\")\n",
    "        result = {\"filename\": filename, \"content\": content}\n",
    "        all_results.append(result)\n",
    "        processed.add(filename)\n",
    "        counter += 1\n",
    "\n",
    "    if counter % SAVE_INTERVAL == 0:\n",
    "        print(f\"Saving {len(all_results)} results and processed set...\")\n",
    "        append_jsonl(\"relations.jsonl\", all_results)\n",
    "        save_processed_set(\"processed.pkl\", processed)\n",
    "        all_results = []\n",
    "\n",
    "\n",
    "if all_results:\n",
    "    print(f\"Saving final {len(all_results)} results and processed set...\")\n",
    "    append_jsonl(\"relations.jsonl\", all_results)\n",
    "    save_processed_set(\"processed.pkl\", processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8eb870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def reduce_paraphrases(frames_dict, paraphrase_clusters, contradictions, problem_name):\n",
    "    output_dir = os.path.join(\"final_frames\", problem_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Clean and validate paraphrase clusters\n",
    "    clean_clusters = []\n",
    "    for cluster in paraphrase_clusters:\n",
    "        valid_cluster = [f for f in cluster if f in frames_dict]\n",
    "        if len(valid_cluster) >= 2:\n",
    "            clean_clusters.append(valid_cluster)\n",
    "\n",
    "    # Clean and validate contradictions\n",
    "    clean_contradictions = []\n",
    "    contradict_set = set()\n",
    "    for x, y in contradictions:\n",
    "        if x in frames_dict and y in frames_dict:\n",
    "            clean_contradictions.append((x, y))\n",
    "            contradict_set.update([x, y])\n",
    "\n",
    "    # Build paraphrase graph\n",
    "    g = nx.Graph()\n",
    "    for cluster in clean_clusters:\n",
    "        for i in range(len(cluster)):\n",
    "            for j in range(i + 1, len(cluster)):\n",
    "                g.add_edge(cluster[i], cluster[j])\n",
    "\n",
    "    representative_map = {}\n",
    "    final_frames = {}\n",
    "\n",
    "    for cluster in nx.connected_components(g):\n",
    "        cluster = list(cluster)\n",
    "        # Prefer a frame involved in a contradiction\n",
    "        contradiction_frames = [f for f in cluster if f in contradict_set]\n",
    "        if contradiction_frames:\n",
    "            representative = contradiction_frames[0]\n",
    "        else:\n",
    "            representative = min(cluster, key=lambda f: len(frames_dict[f][\"frame\"]))\n",
    "        \n",
    "        merged_sources = set()\n",
    "        total_count = 0\n",
    "        for f in cluster:\n",
    "            representative_map[f] = representative\n",
    "            total_count += 1\n",
    "            merged_sources.add(frames_dict[f][\"source\"])\n",
    "        rep_frame = frames_dict[representative].copy()\n",
    "        rep_frame[\"count\"] = total_count\n",
    "        rep_frame[\"sources\"] = list(merged_sources)\n",
    "        final_frames[representative] = rep_frame\n",
    "\n",
    "    # Add isolated frames (not in any cluster)\n",
    "    for fid, frame in frames_dict.items():\n",
    "        if fid not in representative_map:\n",
    "            representative_map[fid] = fid\n",
    "            frame = frame.copy()\n",
    "            frame[\"count\"] = 1\n",
    "            frame[\"sources\"] = [frame[\"source\"]]\n",
    "            final_frames[fid] = frame\n",
    "\n",
    "    print(f'{problem_name} {len(final_frames)}')\n",
    "    # Save final frames\n",
    "    with open(os.path.join(output_dir, \"frames.jsonl\"), \"w\") as fout:\n",
    "        for frame in final_frames.values():\n",
    "            json.dump(frame, fout)\n",
    "            fout.write(\"\\n\")\n",
    "\n",
    "    # Save representative map\n",
    "    with open(os.path.join(output_dir, \"representative_map.json\"), \"w\") as fout:\n",
    "        json.dump(representative_map, fout, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101def5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in read_jsonl('relations.jsonl'):\n",
    "    content=json.loads(line['content'])\n",
    "    paraphrase_clusters = content['relations'][\"paraphrases\"]\n",
    "    contradictions = content['relations'][\"contradictions\"]\n",
    "    problem=line['filename']\n",
    "    problem_path=os.path.join('problem_frames',problem)\n",
    "    frames_dict = {}\n",
    "    with open(problem_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            frame = json.loads(line)\n",
    "            frames_dict[frame[\"id\"]] = frame\n",
    "    reduce_paraphrases(frames_dict, paraphrase_clusters, contradictions, problem.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13731904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def merge_sources_in_frames_jsonl(frames_path):\n",
    "    updated_frames = []\n",
    "    \n",
    "    with open(frames_path, 'r') as f_in:\n",
    "        for line in f_in:\n",
    "            frame = json.loads(line)\n",
    "            # Merge source into sources\n",
    "            sources = set(frame.get(\"sources\", []))\n",
    "            if \"source\" in frame:\n",
    "                sources.add(frame[\"source\"])\n",
    "                del frame[\"source\"]\n",
    "            frame[\"sources\"] = list(sources)\n",
    "            updated_frames.append(frame)\n",
    "\n",
    "    # Overwrite the file with updated frames\n",
    "    with open(frames_path, 'w') as f_out:\n",
    "        for frame in updated_frames:\n",
    "            f_out.write(json.dumps(frame) + \"\\n\")\n",
    "\n",
    "def fix_all_frames_sources(folder=\"final_frames\"):\n",
    "    for problem in os.listdir(folder):\n",
    "        frames_path = os.path.join(folder, problem, \"frames.jsonl\")\n",
    "        if os.path.exists(frames_path):\n",
    "            merge_sources_in_frames_jsonl(frames_path)\n",
    "fix_all_frames_sources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40559a41",
   "metadata": {},
   "source": [
    "Running reduce paraphrases for the second time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342b2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only run relations for two times.\n",
    "\n",
    "SAVE_INTERVAL = 3\n",
    "counter = 0\n",
    "\n",
    "def append_jsonl(filepath, data):\n",
    "    with open(filepath, 'a', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "def save_processed_set(filepath, processed_set):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(processed_set, f)\n",
    "\n",
    "# Initialize\n",
    "all_results = []\n",
    "\n",
    "for f in sorted(os.listdir('final_frames')):\n",
    "    filename=f + '.jsonl'\n",
    "    if (filename in filenames) or (filename in processed):\n",
    "        continue\n",
    "\n",
    "    ffilename = os.path.join('final_frames', f, 'frames.jsonl')\n",
    "    if not os.path.exists(ffilename):\n",
    "        print(f\"Skipping {filename} as frames.jsonl does not exist\")\n",
    "        continue\n",
    "    msg.extend(make_user_prompt(ffilename, user_prompt))\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=config.model,\n",
    "            messages=msg,\n",
    "            temperature=config.temperature,\n",
    "            seed=config.seed,\n",
    "            response_format=config.response_format,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"API call failed for {filename}: {e}\")\n",
    "        msg.pop()\n",
    "        continue\n",
    "\n",
    "    msg.pop()\n",
    "\n",
    "    if not completion:\n",
    "        print(f\"Skipping meme {filename} due to API safety error\")\n",
    "    else:\n",
    "        content = completion.choices[0].message.content\n",
    "        print(f\"{filename} {completion.usage.prompt_tokens_details.cached_tokens}\")\n",
    "        result = {\"filename\": filename, \"content\": content}\n",
    "        all_results.append(result)\n",
    "        processed.add(filename)\n",
    "        counter += 1\n",
    "\n",
    "    if counter % SAVE_INTERVAL == 0:\n",
    "        print(f\"Saving {len(all_results)} results and processed set...\")\n",
    "        append_jsonl(\"relations2.jsonl\", all_results)\n",
    "        save_processed_set(\"processed.pkl\", processed)\n",
    "        all_results = []\n",
    "\n",
    "\n",
    "if all_results:\n",
    "    print(f\"Saving final {len(all_results)} results and processed set...\")\n",
    "    append_jsonl(\"relations3.jsonl\", all_results)\n",
    "    save_processed_set(\"processed.pkl\", processed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39ec33f",
   "metadata": {},
   "source": [
    "all_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1022ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "def update_paraphrase_representatives(paraphrase_clusters, contradictions, problem_name):\n",
    "    problem_dir = os.path.join(\"final_frames\", problem_name)\n",
    "    frames_path = os.path.join(problem_dir, \"frames.jsonl\")\n",
    "    rep_map_path = os.path.join(problem_dir, \"representative_map.json\")\n",
    "\n",
    "    # Load existing frames\n",
    "    frames_dict = {}\n",
    "    with open(frames_path, \"r\") as fin:\n",
    "        for line in fin:\n",
    "            frame = json.loads(line)\n",
    "            frames_dict[frame[\"id\"]] = frame\n",
    "\n",
    "    # Load existing representative map (if any)\n",
    "    if os.path.exists(rep_map_path):\n",
    "        with open(rep_map_path, \"r\") as fin:\n",
    "            representative_map = json.load(fin)\n",
    "    else:\n",
    "        representative_map = {}\n",
    "\n",
    "    # Validate paraphrase clusters\n",
    "    clean_clusters = []\n",
    "    for cluster in paraphrase_clusters:\n",
    "        valid_cluster = [f for f in cluster if f in frames_dict]\n",
    "        if len(valid_cluster) >= 2:\n",
    "            clean_clusters.append(valid_cluster)\n",
    "\n",
    "    # Validate contradiction frames\n",
    "    contradict_set = set()\n",
    "    for item in contradictions:\n",
    "        if isinstance(item, (list, tuple)) and len(item) == 2:\n",
    "            x, y = item\n",
    "            if x in frames_dict and y in frames_dict:\n",
    "                contradict_set.update([x, y])\n",
    "\n",
    "    # Build paraphrase graph\n",
    "    g = nx.Graph()\n",
    "    for cluster in clean_clusters:\n",
    "        for i in range(len(cluster)):\n",
    "            for j in range(i + 1, len(cluster)):\n",
    "                g.add_edge(cluster[i], cluster[j])\n",
    "\n",
    "    updated_frames = {}\n",
    "    clustered_frames = set()\n",
    "\n",
    "    # Process clusters and select representatives\n",
    "    for cluster in nx.connected_components(g):\n",
    "        cluster = list(cluster)\n",
    "        clustered_frames.update(cluster)\n",
    "\n",
    "        # Choose representative\n",
    "        contradiction_frames = [f for f in cluster if f in contradict_set]\n",
    "        if contradiction_frames:\n",
    "            representative = contradiction_frames[0]\n",
    "        else:\n",
    "            representative = min(cluster, key=lambda f: len(frames_dict[f][\"frame\"]))\n",
    "\n",
    "        # Merge metadata\n",
    "        merged_sources = set(frames_dict[representative].get(\"sources\", []))\n",
    "        rationale = frames_dict[representative].get(\"rationale\", [])\n",
    "        count = frames_dict[representative].get(\"count\", 1)\n",
    "\n",
    "        for f in cluster:\n",
    "            representative_map[f] = representative\n",
    "            if f != representative:\n",
    "                frame_data = frames_dict[f]\n",
    "                merged_sources.update(frame_data.get(\"sources\", []))\n",
    "                rationale.extend(frame_data.get(\"rationale\", []))\n",
    "                count += frame_data.get(\"count\", 1)\n",
    "\n",
    "        # Update representative frame\n",
    "        rep_frame = frames_dict[representative].copy()\n",
    "        rep_frame[\"sources\"] = sorted(set(merged_sources))\n",
    "        rep_frame[\"rationale\"] = sorted(set(rationale))\n",
    "        rep_frame[\"count\"] = count\n",
    "        updated_frames[representative] = rep_frame\n",
    "\n",
    "    # Add isolated (non-clustered) frames\n",
    "    for fid, frame in frames_dict.items():\n",
    "        if fid not in clustered_frames:\n",
    "            representative_map[fid] = fid\n",
    "            updated_frames[fid] = frame\n",
    "    print(f\"{problem_name}: {len(updated_frames)} frames written\")\n",
    "    # Write updated representative frames only\n",
    "    with open(frames_path, \"w\") as fout:\n",
    "        for frame in updated_frames.values():\n",
    "            json.dump(frame, fout)\n",
    "            fout.write(\"\\n\")\n",
    "\n",
    "    # Update representative map\n",
    "    with open(rep_map_path, \"w\") as fout:\n",
    "        json.dump(representative_map, fout, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b454668",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in read_jsonl('relations3.jsonl'):\n",
    "    content = json.loads(line['content'])\n",
    "    paraphrase_clusters = content['relations'].get(\"paraphrases\", [])\n",
    "    contradictions = content['relations'].get(\"contradictions\", [])\n",
    "    problem = line['filename'].split('.')[0]\n",
    "    print(f\"{problem} done\")\n",
    "    update_paraphrase_representatives(\n",
    "        paraphrase_clusters=paraphrase_clusters,\n",
    "        contradictions=contradictions,\n",
    "        problem_name=problem\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "624ca109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems present in 'problem_frames' but missing in 'final_frames':\n",
      "- classism\n",
      "- commitment_phobia\n",
      "- conspiracy_thinking\n",
      "- disrespect_towards_women\n",
      "- endorsing_necrophilia\n",
      "- exploitation_of_women_by_men\n",
      "- forced_marriage\n",
      "- gatekeeping\n",
      "- homophobia\n",
      "- lack_of_accountability_by_men\n",
      "- male_validation\n",
      "- neo_sexism\n",
      "- professional_misconduct\n",
      "- promoting_self_harm\n",
      "- single_motherhood_stigmatization\n",
      "- toxic_masculinity\n",
      "- trivializing_addiction\n",
      "- trivializing_eating_disorders\n",
      "- trivializing_the_need_for_representation\n",
      "- wage_disparity\n",
      "- workplace_discrimination\n"
     ]
    }
   ],
   "source": [
    "final_folders = set(os.listdir('final_frames'))\n",
    "problem_folders = set(f.split('.')[0] for f in os.listdir('problem_frames'))\n",
    "\n",
    "missing_in_final = problem_folders - final_folders\n",
    "\n",
    "print(\"Problems present in 'problem_frames' but missing in 'final_frames':\")\n",
    "for problem in sorted(missing_in_final):\n",
    "    print(\"-\", problem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b56545",
   "metadata": {},
   "source": [
    "Getting similar frames using sentence transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6459a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridicule: 332 frames\n",
      "reductionism: 269 frames\n",
      "derogatory_labeling: 225 frames\n",
      "objectification: 221 frames\n",
      "stereotyping: 210 frames\n",
      "sexual_innuendo: 206 frames\n",
      "minimizing_feminist_efforts: 126 frames\n",
      "double_standards: 102 frames\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "base_dir = \"final_frames\"\n",
    "threshold = 100\n",
    "problems_names = []\n",
    "\n",
    "for problem in os.listdir(base_dir):\n",
    "    frames_path = os.path.join(base_dir, problem, \"frames.jsonl\")\n",
    "    if not os.path.exists(frames_path):\n",
    "        continue\n",
    "\n",
    "    with open(frames_path, \"r\") as f:\n",
    "        count = sum(1 for _ in f)\n",
    "\n",
    "    if count > threshold:\n",
    "        problems_names.append((problem, count))\n",
    "\n",
    "# Print results\n",
    "for prob, cnt in sorted(problems_names, key=lambda x: -x[1]):\n",
    "    print(f\"{prob}: {cnt} frames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55fe1d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "problems_names=['promoting_rape_culture','misrepresentation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ad739566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def read_jsonl(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                ex = json.loads(line)\n",
    "                yield ex\n",
    "\n",
    "def write_jsonl(path, data):\n",
    "    with open(path, \"w\") as f:\n",
    "        for ex in data:\n",
    "            f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "contradictions = defaultdict(set)\n",
    "for line in read_jsonl('relations.jsonl'):\n",
    "    content = json.loads(line['content'])\n",
    "    contradictions_list = content['relations'].get(\"contradictions\", [])\n",
    "    problem = line['filename'].split('.')[0]\n",
    "    for item in contradictions_list:\n",
    "        if isinstance(item, (list, tuple)) and len(item) == 2:\n",
    "            x, y = item\n",
    "            contradictions[problem].add(x)\n",
    "            contradictions[problem].add(y)\n",
    "for line in read_jsonl('relations2.jsonl'):\n",
    "    content = json.loads(line['content'])\n",
    "    contradictions_list = content['relations'].get(\"contradictions\", [])\n",
    "    problem = line['filename'].split('.')[0]\n",
    "    for item in contradictions_list:\n",
    "        if isinstance(item, (list, tuple)) and len(item) == 2:\n",
    "            x, y = item\n",
    "            contradictions[problem].add(x)\n",
    "            contradictions[problem].add(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "90ecfad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contradictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e8b3cd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 74/74 [00:00<00:00, 112.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Frame counts in paraphrase clusters:\n",
      "misrepresentation: 19 frames in clusters\n",
      "promoting_rape_culture: 12 frames in clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from sentence_transformers.util import cos_sim\n",
    "from tqdm import tqdm\n",
    "\n",
    "SIMILARITY_THRESHOLD = 0.5\n",
    "FRAME_DIR = \"final_frames\"\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def load_frames(path):\n",
    "    frames = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            frames.append(json.loads(line))\n",
    "    return frames\n",
    "\n",
    "def find_paraphrase_clusters(frames, threshold=SIMILARITY_THRESHOLD):\n",
    "    texts = [frame[\"frame\"] for frame in frames]\n",
    "    ids = [frame[\"id\"] for frame in frames]\n",
    "    embeddings = model.encode(texts, convert_to_tensor=True)\n",
    "\n",
    "    clustered = set()\n",
    "    paraphrase_clusters = []\n",
    "\n",
    "    for i in range(len(ids)):\n",
    "        if ids[i] in clustered:\n",
    "            continue\n",
    "        cluster = [ids[i]]\n",
    "        clustered.add(ids[i])\n",
    "        for j in range(i + 1, len(ids)):\n",
    "            if ids[j] in clustered:\n",
    "                continue\n",
    "            sim = cos_sim(embeddings[i], embeddings[j]).cpu().item()\n",
    "            if sim >= threshold:\n",
    "                cluster.append(ids[j])\n",
    "                clustered.add(ids[j])\n",
    "        if len(cluster) > 1:\n",
    "            paraphrase_clusters.append(cluster)\n",
    "\n",
    "    return paraphrase_clusters\n",
    "\n",
    "def process_all_problems(problems_names):\n",
    "    count_summary = {}\n",
    "    all_clusters = {}\n",
    "\n",
    "    for problem in tqdm(os.listdir(FRAME_DIR)):\n",
    "        frames_path = os.path.join(FRAME_DIR, problem, \"frames.jsonl\")\n",
    "        if not os.path.exists(frames_path):\n",
    "            continue\n",
    "\n",
    "        frames = load_frames(frames_path)\n",
    "        if problem not in problems_names:\n",
    "            continue\n",
    "\n",
    "        contradiction_ids = contradictions.get(problem, set())\n",
    "        filtered_frames = [frame for frame in frames if frame[\"id\"] not in contradiction_ids]\n",
    "\n",
    "        clusters = find_paraphrase_clusters(filtered_frames)\n",
    "\n",
    "        total_clustered = len({fid for cluster in clusters for fid in cluster})\n",
    "        count_summary[problem] = total_clustered\n",
    "        all_clusters[problem] = clusters\n",
    "\n",
    "    print(\"\\nðŸ” Frame counts in paraphrase clusters:\")\n",
    "    for problem, count in sorted(count_summary.items(), key=lambda x: -x[1]):\n",
    "        print(f\"{problem}: {count} frames in clusters\")\n",
    "\n",
    "    return all_clusters\n",
    "\n",
    "all_clusters = process_all_problems(problems_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7947120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_paraphrase_clusters(all_clusters, frame_dir=FRAME_DIR, max_problems=None, max_clusters_per_problem=None):\n",
    "    \"\"\"\n",
    "    Print all paraphrase clusters with full frame texts for each problem.\n",
    "\n",
    "    Args:\n",
    "        all_clusters (dict): Dictionary of {problem_name: list of paraphrase clusters (frame ID lists)}.\n",
    "        frame_dir (str): Path to the folder where each problem has its frames.jsonl file.\n",
    "        max_problems (int, optional): Limit number of problems to print.\n",
    "        max_clusters_per_problem (int, optional): Limit number of clusters printed per problem.\n",
    "    \"\"\"\n",
    "    for idx, (problem, clusters) in enumerate(all_clusters.items()):\n",
    "        if max_problems is not None and idx >= max_problems:\n",
    "            break\n",
    "\n",
    "        print(f\"\\nðŸ§© Problem: {problem} ({len(clusters)} clusters)\")\n",
    "        frames_path = os.path.join(frame_dir, problem, \"frames.jsonl\")\n",
    "        if not os.path.exists(frames_path):\n",
    "            print(f\"âš ï¸  Missing frames.jsonl for {problem}\")\n",
    "            continue\n",
    "\n",
    "        # Load frame ID â†’ text mapping\n",
    "        frame_dict = {}\n",
    "        with open(frames_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                data = json.loads(line)\n",
    "                frame_dict[data[\"id\"]] = data[\"frame\"]\n",
    "\n",
    "        for c_idx, cluster in enumerate(clusters):\n",
    "            if max_clusters_per_problem is not None and c_idx >= max_clusters_per_problem:\n",
    "                break\n",
    "\n",
    "            print(f\"\\nðŸ“Œ Cluster {c_idx + 1} ({len(cluster)} frames):\")\n",
    "            for fid in cluster:\n",
    "                frame_text = frame_dict.get(fid, \"[Missing Frame]\")\n",
    "                print(f\"  - [{fid}]: {frame_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c88dd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(problems_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ad2b1154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promoting_rape_culture: 24 frames written\n",
      "misrepresentation: 21 frames written\n"
     ]
    }
   ],
   "source": [
    "for name in problems_names:\n",
    "    update_paraphrase_representatives(all_clusters[name], [], name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e02505d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(all_clusters.values())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae89d9",
   "metadata": {},
   "source": [
    "Running paraphrases in GPT one last time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fec34355",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = 'prompts/relations2.yaml'\n",
    "with open(config_file_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    config = ChatCompletionConfig(**config)\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'], timeout=90)\n",
    "sys_prompt = config.system_prompt.strip()\n",
    "user_prompt= config.user_prompt.strip()\n",
    "processed=set()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfece013",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg=[{\"role\": \"system\", \"content\": sys_prompt}]\n",
    "for demo in demos:\n",
    "    msg.extend(make_user_prompt(demo['filename'],user_prompt, True, demo['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "458805fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n",
      "Saving 1 results and processed set...\n"
     ]
    }
   ],
   "source": [
    "#only run relations for two times.\n",
    "\n",
    "SAVE_INTERVAL = 1\n",
    "counter = 0\n",
    "\n",
    "def append_jsonl(filepath, data):\n",
    "    with open(filepath, 'a', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "def save_processed_set(filepath, processed_set):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(processed_set, f)\n",
    "\n",
    "# Initialize\n",
    "all_results = []\n",
    "to_process=['unrealistic_beauty_standards','victim_blaming','undermining_women_s_rights_movements','undermining_women_s_capabilities','trivializing_women_s_sexual_satisfaction',\n",
    "            'trivializing_women_s_issues','trivializing_sexual_assault', 'transactional_relationships','stigmatization','sexual_innuendo','sexual_entitlement','scapegoating',\n",
    "            'promoting_rape_culture', 'policing_women_s_bodies','ownership','misunderstanding_feminism','misrepresentation','minimizing_feminist_efforts',\n",
    "            'ridicule','reductionism','reverse_sexism','invalidating_women_s_experiences','intersectional_prejudice','intellectual_degradation','gender_essentialism','fat_shaming','false_equivalence',\n",
    "            'exclusion','enforced_gender_norms','double_standards','divisiveness','dismissiveness','derogatory_labeling','demeaning_aspirations','deflection','cultural_insensitivity',\n",
    "            'age_related_sexualization', 'coercion']\n",
    "for f in to_process:\n",
    "    ffilename = os.path.join('final_frames', f, 'frames.jsonl')\n",
    "    if not os.path.exists(ffilename):\n",
    "        print(f\"Skipping {filename} as frames.jsonl does not exist\")\n",
    "        continue\n",
    "    msg.extend(make_user_prompt(ffilename, user_prompt))\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=config.model,\n",
    "            messages=msg,\n",
    "            temperature=config.temperature,\n",
    "            seed=config.seed,\n",
    "            response_format=config.response_format,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"API call failed for {f}: {e}\")\n",
    "        msg.pop()\n",
    "        continue\n",
    "\n",
    "    msg.pop()\n",
    "\n",
    "    if not completion:\n",
    "        print(f\"Skipping meme {f}due to API safety error\")\n",
    "    else:\n",
    "        content = completion.choices[0].message.content\n",
    "        result = {\"filename\": f, \"content\": content}\n",
    "        all_results.append(result)\n",
    "        processed.add(f)\n",
    "        counter += 1\n",
    "\n",
    "    if counter % SAVE_INTERVAL == 0:\n",
    "        print(f\"Saving {len(all_results)} results and processed set...\")\n",
    "        append_jsonl(\"relations6.jsonl\", all_results)\n",
    "        save_processed_set(\"processed.pkl\", processed)\n",
    "        all_results = []\n",
    "\n",
    "\n",
    "if all_results:\n",
    "    print(f\"Saving final {len(all_results)} results and processed set...\")\n",
    "    append_jsonl(\"relations6.jsonl\", all_results)\n",
    "    save_processed_set(\"processed.pkl\", processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f042bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "contradictions = defaultdict(list)\n",
    "for line in read_jsonl('relations.jsonl'):\n",
    "    content = json.loads(line['content'])\n",
    "    contradictions_list = content['relations'].get(\"contradictions\", [])\n",
    "    problem = line['filename'].split('.')[0]\n",
    "    for item in contradictions_list:\n",
    "        if isinstance(item, (list, tuple)) and len(item) == 2:\n",
    "            contradictions[problem].append(item)\n",
    "\n",
    "for line in read_jsonl('relations2.jsonl'):\n",
    "    content = json.loads(line['content'])\n",
    "    contradictions_list = content['relations'].get(\"contradictions\", [])\n",
    "    problem = line['filename'].split('.')[0]\n",
    "    for item in contradictions_list:\n",
    "        if isinstance(item, (list, tuple)) and len(item) == 2:\n",
    "            contradictions[problem].append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "contradictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3840d7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unrealistic_beauty_standards done\n",
      "unrealistic_beauty_standards: 15 frames written\n",
      "victim_blaming done\n",
      "victim_blaming: 13 frames written\n",
      "undermining_women_s_rights_movements done\n",
      "undermining_women_s_rights_movements: 9 frames written\n",
      "undermining_women_s_capabilities done\n",
      "undermining_women_s_capabilities: 15 frames written\n",
      "trivializing_women_s_sexual_satisfaction done\n",
      "trivializing_women_s_sexual_satisfaction: 7 frames written\n",
      "trivializing_women_s_issues done\n",
      "trivializing_women_s_issues: 8 frames written\n",
      "trivializing_sexual_assault done\n",
      "trivializing_sexual_assault: 6 frames written\n",
      "transactional_relationships done\n",
      "transactional_relationships: 7 frames written\n",
      "stigmatization done\n",
      "stigmatization: 10 frames written\n",
      "sexual_innuendo done\n",
      "sexual_innuendo: 20 frames written\n",
      "sexual_entitlement done\n",
      "sexual_entitlement: 20 frames written\n",
      "scapegoating done\n",
      "scapegoating: 15 frames written\n",
      "promoting_rape_culture done\n",
      "promoting_rape_culture: 12 frames written\n",
      "policing_women_s_bodies done\n",
      "policing_women_s_bodies: 8 frames written\n",
      "ownership done\n",
      "ownership: 4 frames written\n",
      "misunderstanding_feminism done\n",
      "misunderstanding_feminism: 14 frames written\n",
      "misrepresentation done\n",
      "misrepresentation: 17 frames written\n",
      "minimizing_feminist_efforts done\n",
      "minimizing_feminist_efforts: 13 frames written\n",
      "ridicule done\n",
      "ridicule: 44 frames written\n",
      "reductionism done\n",
      "reductionism: 35 frames written\n",
      "reverse_sexism done\n",
      "reverse_sexism: 15 frames written\n",
      "invalidating_women_s_experiences done\n",
      "invalidating_women_s_experiences: 13 frames written\n",
      "intersectional_prejudice done\n",
      "intersectional_prejudice: 15 frames written\n",
      "intellectual_degradation done\n",
      "intellectual_degradation: 10 frames written\n",
      "gender_essentialism done\n",
      "gender_essentialism: 15 frames written\n",
      "fat_shaming done\n",
      "fat_shaming: 15 frames written\n",
      "false_equivalence done\n",
      "false_equivalence: 16 frames written\n",
      "exclusion done\n",
      "exclusion: 15 frames written\n",
      "enforced_gender_norms done\n",
      "enforced_gender_norms: 11 frames written\n",
      "double_standards done\n",
      "double_standards: 29 frames written\n",
      "divisiveness done\n",
      "divisiveness: 11 frames written\n",
      "dismissiveness done\n",
      "dismissiveness: 12 frames written\n",
      "derogatory_labeling done\n",
      "derogatory_labeling: 25 frames written\n",
      "demeaning_aspirations done\n",
      "demeaning_aspirations: 11 frames written\n",
      "deflection done\n",
      "deflection: 10 frames written\n",
      "cultural_insensitivity done\n",
      "cultural_insensitivity: 6 frames written\n",
      "age_related_sexualization done\n",
      "age_related_sexualization: 6 frames written\n",
      "coercion done\n",
      "coercion: 8 frames written\n"
     ]
    }
   ],
   "source": [
    "for line in read_jsonl('relations6.jsonl'):\n",
    "    content = json.loads(line['content'])\n",
    "    problem = line['filename']\n",
    "    paraphrase_clusters = content['relations'].get(\"paraphrases\", [])\n",
    "    contradictions_final=content['relations'].get(\"contradictions\", [])\n",
    "    contradictions_final.extend(contradictions[problem])\n",
    "    print(f\"{problem} done\")\n",
    "    update_paraphrase_representatives(\n",
    "        paraphrase_clusters=paraphrase_clusters,\n",
    "        contradictions=contradictions_final,\n",
    "        problem_name=problem\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79044e9",
   "metadata": {},
   "source": [
    "Getting manually curated relations to final frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21443615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sexist_pseudoscience 3\n",
      "trivializing_addiction 5\n",
      "endorsing_necrophilia 5\n",
      "neo_sexism 7\n",
      "classism 8\n",
      "gatekeeping 6\n",
      "commitment_phobia 11\n",
      "workplace_discrimination 9\n",
      "disrespect_towards_women 3\n",
      "conspiracy_thinking 6\n",
      "exploitation_of_women_by_men 1\n",
      "forced_marriage 2\n",
      "homophobia 5\n",
      "lack_of_accountability_by_men 2\n",
      "professional_misconduct 1\n",
      "single_motherhood_stigmatization 7\n",
      "toxic_masculinity 5\n",
      "trivializing_eating_disorders 2\n",
      "trivializing_the_need_for_representation 4\n",
      "wage_disparity 3\n",
      "promoting_self_harm 6\n"
     ]
    }
   ],
   "source": [
    "for line in read_jsonl('relations7.jsonl'):\n",
    "    content=json.loads(line['content'])\n",
    "    paraphrase_clusters = content['relations'][\"paraphrases\"]\n",
    "    contradictions = content['relations'][\"contradictions\"]\n",
    "    problem=line['filename'] + '.jsonl'\n",
    "    problem_path=os.path.join('problem_frames',problem)\n",
    "    frames_dict = {}\n",
    "    with open(problem_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            frame = json.loads(line)\n",
    "            frames_dict[frame[\"id\"]] = frame\n",
    "    reduce_paraphrases(frames_dict, paraphrase_clusters, contradictions, problem.split('.')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08e88b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems present in 'problem_frames' but missing in 'final_frames':\n"
     ]
    }
   ],
   "source": [
    "final_folders = set(os.listdir('final_frames'))\n",
    "problem_folders = set(f.split('.')[0] for f in os.listdir('problem_frames'))\n",
    "\n",
    "missing_in_final = problem_folders - final_folders\n",
    "\n",
    "print(\"Problems present in 'problem_frames' but missing in 'final_frames':\")\n",
    "for problem in sorted(missing_in_final):\n",
    "    print(\"-\", problem)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a9eb0",
   "metadata": {},
   "source": [
    "Correcting sources for one last time for all frames in final_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55dafe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sources_in_frames_jsonl(frames_path):\n",
    "    updated_frames = []\n",
    "    \n",
    "    with open(frames_path, 'r') as f_in:\n",
    "        for line in f_in:\n",
    "            frame = json.loads(line)\n",
    "            # Merge source into sources\n",
    "            sources = set(frame.get(\"sources\", []))\n",
    "            if \"source\" in frame:\n",
    "                sources.add(frame[\"source\"])\n",
    "                del frame[\"source\"]\n",
    "            frame[\"sources\"] = list(sources)\n",
    "            updated_frames.append(frame)\n",
    "\n",
    "    # Overwrite the file with updated frames\n",
    "    with open(frames_path, 'w') as f_out:\n",
    "        for frame in updated_frames:\n",
    "            f_out.write(json.dumps(frame) + \"\\n\")\n",
    "\n",
    "def fix_all_frames_sources(folder=\"final_frames\"):\n",
    "    for problem in os.listdir(folder):\n",
    "        frames_path = os.path.join(folder, problem, \"frames.jsonl\")\n",
    "        if os.path.exists(frames_path):\n",
    "            merge_sources_in_frames_jsonl(frames_path)\n",
    "fix_all_frames_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1453e446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "x=sum([1 for _ in os.listdir('final_frames')])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96d18958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ableism: 8 unique frame(s)\n",
      "ageism: 13 unique frame(s)\n",
      "age_related_sexualization: 6 unique frame(s)\n",
      "appearance_based_discrimination: 19 unique frame(s)\n",
      "biased_judgement: 14 unique frame(s)\n",
      "classism: 8 unique frame(s)\n",
      "coercion: 8 unique frame(s)\n",
      "commitment_phobia: 11 unique frame(s)\n",
      "conditional_respect: 10 unique frame(s)\n",
      "conspiracy_thinking: 6 unique frame(s)\n",
      "cultural_insensitivity: 6 unique frame(s)\n",
      "deflection: 10 unique frame(s)\n",
      "dehumanization_of_women: 22 unique frame(s)\n",
      "demeaning_aspirations: 11 unique frame(s)\n",
      "demonization: 11 unique frame(s)\n",
      "derogatory_labeling: 25 unique frame(s)\n",
      "discrimination: 1 unique frame(s)\n",
      "dismissing_women_s_rights: 11 unique frame(s)\n",
      "dismissiveness: 12 unique frame(s)\n",
      "disposability: 10 unique frame(s)\n",
      "disrespecting_sex_workers: 9 unique frame(s)\n",
      "disrespect_towards_women: 3 unique frame(s)\n",
      "divisiveness: 11 unique frame(s)\n",
      "double_standards: 29 unique frame(s)\n",
      "endorsing_marital_rape: 3 unique frame(s)\n",
      "endorsing_necrophilia: 5 unique frame(s)\n",
      "enforced_gender_norms: 11 unique frame(s)\n",
      "exclusion: 15 unique frame(s)\n",
      "exploitation_of_women_by_men: 1 unique frame(s)\n",
      "false_accusations: 13 unique frame(s)\n",
      "false_equivalence: 16 unique frame(s)\n",
      "fat_shaming: 15 unique frame(s)\n",
      "fearmongering: 6 unique frame(s)\n",
      "forced_marriage: 2 unique frame(s)\n",
      "gatekeeping: 6 unique frame(s)\n",
      "gender_essentialism: 15 unique frame(s)\n",
      "homophobia: 5 unique frame(s)\n",
      "incest: 6 unique frame(s)\n",
      "intellectual_degradation: 10 unique frame(s)\n",
      "intersectional_prejudice: 15 unique frame(s)\n",
      "invalidating_women_s_experiences: 13 unique frame(s)\n",
      "lack_of_accountability_by_men: 2 unique frame(s)\n",
      "male_validation: 5 unique frame(s)\n",
      "menstruation_stigma: 6 unique frame(s)\n",
      "minimizing_feminist_efforts: 13 unique frame(s)\n",
      "misrepresentation: 17 unique frame(s)\n",
      "mistrust_in_women: 8 unique frame(s)\n",
      "misunderstanding_feminism: 14 unique frame(s)\n",
      "neo_sexism: 7 unique frame(s)\n",
      "objectification: 137 unique frame(s)\n",
      "ownership: 4 unique frame(s)\n",
      "patriarchal_attitudes: 9 unique frame(s)\n",
      "patriarchal_control: 11 unique frame(s)\n",
      "pedophilia_exploitation: 7 unique frame(s)\n",
      "policing_women_s_bodies: 8 unique frame(s)\n",
      "possessiveness: 5 unique frame(s)\n",
      "professional_misconduct: 1 unique frame(s)\n",
      "promoting_infidelity: 4 unique frame(s)\n",
      "promoting_rape_culture: 12 unique frame(s)\n",
      "promoting_self_harm: 6 unique frame(s)\n",
      "racism: 12 unique frame(s)\n",
      "reductionism: 35 unique frame(s)\n",
      "reverse_sexism: 15 unique frame(s)\n",
      "ridicule: 44 unique frame(s)\n",
      "scapegoating: 15 unique frame(s)\n",
      "sexist_pseudoscience: 3 unique frame(s)\n",
      "sexualization: 21 unique frame(s)\n",
      "sexual_entitlement: 20 unique frame(s)\n",
      "sexual_harassment: 6 unique frame(s)\n",
      "sexual_innuendo: 20 unique frame(s)\n",
      "single_motherhood_stigmatization: 7 unique frame(s)\n",
      "stereotyping: 111 unique frame(s)\n",
      "stigmatization: 10 unique frame(s)\n",
      "toxic_masculinity: 5 unique frame(s)\n",
      "transactional_relationships: 7 unique frame(s)\n",
      "transphobia: 10 unique frame(s)\n",
      "trivializing_addiction: 5 unique frame(s)\n",
      "trivializing_consent: 7 unique frame(s)\n",
      "trivializing_eating_disorders: 2 unique frame(s)\n",
      "trivializing_infidelity: 5 unique frame(s)\n",
      "trivializing_mental_health_issues: 6 unique frame(s)\n",
      "trivializing_oppression: 1 unique frame(s)\n",
      "trivializing_prostitution: 8 unique frame(s)\n",
      "trivializing_serious_issues: 5 unique frame(s)\n",
      "trivializing_sexual_assault: 6 unique frame(s)\n",
      "trivializing_the_need_for_representation: 4 unique frame(s)\n",
      "trivializing_women_s_issues: 8 unique frame(s)\n",
      "trivializing_women_s_sexual_satisfaction: 7 unique frame(s)\n",
      "undermining_women_s_capabilities: 15 unique frame(s)\n",
      "undermining_women_s_rights_movements: 9 unique frame(s)\n",
      "unrealistic_beauty_standards: 15 unique frame(s)\n",
      "victim_blaming: 13 unique frame(s)\n",
      "violence: 34 unique frame(s)\n",
      "wage_disparity: 3 unique frame(s)\n",
      "women_subjugation: 4 unique frame(s)\n",
      "women_valuation: 5 unique frame(s)\n",
      "workplace_discrimination: 9 unique frame(s)\n",
      "\n",
      "Total number of unique final frames across all problems: 1105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1105"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def count_total_final_frames(final_frames_dir=\"final_frames\"):\n",
    "    unique_ids = set()\n",
    "    per_problem_counts = {}\n",
    "\n",
    "    for problem in os.listdir(final_frames_dir):\n",
    "        problem_path = os.path.join(final_frames_dir, problem, \"frames.jsonl\")\n",
    "        if os.path.isfile(problem_path):\n",
    "            with open(problem_path, 'r') as f:\n",
    "                ids_in_problem = set()\n",
    "                for line in f:\n",
    "                    try:\n",
    "                        frame = json.loads(line)\n",
    "                        frame_id = frame.get(\"id\")\n",
    "                        if frame_id:\n",
    "                            unique_ids.add(frame_id)\n",
    "                            ids_in_problem.add(frame_id)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue  # skip malformed lines\n",
    "                per_problem_counts[problem] = len(ids_in_problem)\n",
    "\n",
    "    for problem, count in per_problem_counts.items():\n",
    "        print(f\"{problem}: {count} unique frame(s)\")\n",
    "\n",
    "    print(f\"\\nTotal number of unique final frames across all problems: {len(unique_ids)}\")\n",
    "    return len(unique_ids)\n",
    "\n",
    "count_total_final_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc32516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "def",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
