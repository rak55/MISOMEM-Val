{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02494351",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI, BadRequestError\n",
    "from openai.types.chat import ChatCompletion\n",
    "from tenacity import retry, stop_after_attempt, wait_random_exponential\n",
    "import time\n",
    "from typing import Optional\n",
    "import base64\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import dataclasses\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "os.environ['OPENAI_API_KEY'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "144e2a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimumDelay:\n",
    "    def __init__(self, delay: float | int):\n",
    "        self.delay = delay\n",
    "        self.start = None\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        end = time.time()\n",
    "        seconds = end - self.start\n",
    "        if self.delay > seconds:\n",
    "            time.sleep(self.delay - seconds)\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=90), stop=stop_after_attempt(3))\n",
    "def chat(client: OpenAI, delay: float | int, **kwargs) -> ChatCompletion | None:\n",
    "    try:\n",
    "        with MinimumDelay(delay):\n",
    "            return client.chat.completions.create(**kwargs)\n",
    "    except BadRequestError as e:\n",
    "        print(f\"Bad Request: {e}\")\n",
    "        if \"safety\" in e.message:\n",
    "            return None\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        print(f\"Exception: {e}\")\n",
    "        raise e\n",
    "def print_messages(messages):\n",
    "    for message in messages:\n",
    "        if isinstance(message[\"content\"], list):\n",
    "            print(f\"{message['role']}:\")\n",
    "            for content in message[\"content\"]:\n",
    "                if content[\"type\"] == \"text\":\n",
    "                    print(content[\"text\"])\n",
    "                elif content[\"type\"] == \"image_url\":\n",
    "                    print(\"[IMAGE]\")\n",
    "        else:\n",
    "            print(f\"{message['role']}: {message['content']}\")\n",
    "        print()\n",
    "    print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8556bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                ex = json.loads(line)\n",
    "                yield ex\n",
    "\n",
    "def write_jsonl(path, data):\n",
    "    with open(path, \"w\") as f:\n",
    "        for ex in data:\n",
    "            f.write(json.dumps(ex) + \"\\n\")\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6797fe70",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class ChatCompletionConfig:\n",
    "    seed: int\n",
    "    delay: int\n",
    "    model: str\n",
    "    max_tokens: int\n",
    "    temperature: float\n",
    "    system_prompt: str\n",
    "    user_prompt: str\n",
    "    response_format: dict | None = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bc7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demos\n",
    "demos=[{'filename': '10089.jpg', 'result':{'frame_problems':[\n",
    "    {'frame':'Feminism is not based on facts or true equality.',\n",
    "    'problem':'Misunderstanding feminism',\n",
    "    'rationale': 'The problem of misunderstanding feminism arises as the meme falsely portrays feminists as shocked or opposed to facts, logic, and actual equality,misrepresenting and dismissing the core principles of feminism.'}\n",
    "]}},\n",
    "{'filename': }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "765cf966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../articulation-annotations.jsonl...\n",
      "Reading misogyny_problems.jsonl...\n",
      "Writing merged data to total.jsonl...\n",
      "Merge complete! 5000 unique entries written to total.jsonl\n",
      "Statistics:\n",
      "  Entries from both files: 5000\n",
      "  Entries only in ../articulation-annotations.jsonl: 0\n",
      "  Entries only in misogyny_problems.jsonl: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "def merge_jsonl_files(file1_path, file2_path, output_path, id_key1='id', id_key2='filename'):\n",
    "    \"\"\"\n",
    "    Merge two JSONL files based on filename/id fields.\n",
    "    \n",
    "    Args:\n",
    "        file1_path: Path to first JSONL file\n",
    "        file2_path: Path to second JSONL file  \n",
    "        output_path: Path for merged output file\n",
    "        id_key1: Key name for filename in first file (default: 'id')\n",
    "        id_key2: Key name for filename in second file (default: 'filename')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dictionary to store merged data by filename\n",
    "    merged_data = defaultdict(dict)\n",
    "    \n",
    "    # Read first file\n",
    "    print(f\"Reading {file1_path}...\")\n",
    "    with open(file1_path, 'r') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                filename = data.get(id_key1)\n",
    "                \n",
    "                if filename:\n",
    "                    # Store all data from file1, including the id field\n",
    "                    merged_data[filename].update(data)\n",
    "                    # Ensure we have a consistent 'filename' field\n",
    "                    merged_data[filename]['filename'] = filename\n",
    "                else:\n",
    "                    print(f\"Warning: No '{id_key1}' field found in line {line_num} of {file1_path}\")\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing line {line_num} in {file1_path}: {e}\")\n",
    "    \n",
    "    # Read second file and merge\n",
    "    print(f\"Reading {file2_path}...\")\n",
    "    with open(file2_path, 'r') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                filename = data.get(id_key2)\n",
    "                \n",
    "                if filename:\n",
    "                    # Merge data from file2, file2 data will overwrite file1 data for same keys\n",
    "                    merged_data[filename].update(data)\n",
    "                    # Ensure we have a consistent 'filename' field\n",
    "                    merged_data[filename]['filename'] = filename\n",
    "                else:\n",
    "                    print(f\"Warning: No '{id_key2}' field found in line {line_num} of {file2_path}\")\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing line {line_num} in {file2_path}: {e}\")\n",
    "    \n",
    "    # Write merged data to output file\n",
    "    print(f\"Writing merged data to {output_path}...\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        for filename in sorted(merged_data.keys()):\n",
    "            final_entry = merged_data[filename].copy()\n",
    "            if 'id' in final_entry:\n",
    "                del final_entry['id']\n",
    "            \n",
    "            json.dump(final_entry, f)\n",
    "            f.write('\\n')\n",
    "    print(f\"Merge complete! {len(merged_data)} unique entries written to {output_path}\")\n",
    "    \n",
    "    # Print some statistics\n",
    "    file1_only = 0\n",
    "    file2_only = 0\n",
    "    both_files = 0\n",
    "    \n",
    "    for filename, data in merged_data.items():\n",
    "        has_file1_data = id_key1 in data\n",
    "        has_file2_data = id_key2 in data or any(key != id_key1 and key != 'filename' for key in data)\n",
    "        \n",
    "        if has_file1_data and has_file2_data:\n",
    "            both_files += 1\n",
    "        elif has_file1_data:\n",
    "            file1_only += 1\n",
    "        elif has_file2_data:\n",
    "            file2_only += 1\n",
    "    \n",
    "    print(f\"Statistics:\")\n",
    "    print(f\"  Entries from both files: {both_files}\")\n",
    "    print(f\"  Entries only in {file1_path}: {file1_only}\")\n",
    "    print(f\"  Entries only in {file2_path}: {file2_only}\")\n",
    "\n",
    "\n",
    "    \n",
    "merge_jsonl_files('../articulation-annotations.jsonl', 'misogyny_problems.jsonl', 'total.jsonl', id_key1='id', id_key2='filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ce9038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files with no problems: 42\n",
      "Number of files with no problems: 66\n"
     ]
    }
   ],
   "source": [
    "def analyze_problems(input_file):\n",
    "    \n",
    "    # Initialize counters\n",
    "    empty_content = 0\n",
    "    empty_problems = 0\n",
    "    empty_filenames= []\n",
    "    \n",
    "    for entry in read_jsonl(input_file):\n",
    "        filename = entry.get(\"filename\", \"unknown\")\n",
    "        content = entry.get(\"content\", \"\")\n",
    "        \n",
    "        # Count empty content\n",
    "        if content == \"\" or content is None:\n",
    "            empty_content += 1\n",
    "            empty_filenames.append(filename)\n",
    "            continue\n",
    "        \n",
    "        # Try to parse the content as JSON\n",
    "        try:\n",
    "            problems_data = json.loads(content)\n",
    "            identified_problems = problems_data.get(\"identified_problems\", [])\n",
    "            \n",
    "            # Count empty problem lists\n",
    "            if len(identified_problems) == 0:\n",
    "                empty_problems += 1\n",
    "                empty_filenames.append(filename)\n",
    "                \n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            empty_filenames.append(filename)\n",
    "\n",
    "    no_problems = empty_content + empty_problems\n",
    "    print(f\"Number of files with no problems: {no_problems}\")\n",
    "    return empty_filenames\n",
    "\n",
    "def analyze_frames(input_file):\n",
    "    \n",
    "    # Initialize counters\n",
    "    empty_content = 0\n",
    "    empty_filenames= []\n",
    "    \n",
    "    for entry in read_jsonl(input_file):\n",
    "        filename = entry.get(\"id\", \"unknown\")\n",
    "        content = entry.get(\"articulations\", \"\")\n",
    "        \n",
    "        # Count empty content\n",
    "        if content == \"\" or content is None or len(content)==0:\n",
    "            empty_content += 1\n",
    "            empty_filenames.append(filename)\n",
    "    print(f\"Number of files with no problems: {empty_content}\")\n",
    "    return empty_filenames\n",
    "\n",
    "no_frames=analyze_frames('../articulation-annotations.jsonl')\n",
    "no_problems=analyze_problems('misogyny_problems.jsonl')\n",
    "no_frames.extend(no_problems)\n",
    "no_total=set(no_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dd6c6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#demos for idea2\n",
    "demos2=[{'filename': '10032.jpg', 'result':{'frame_problems':[\n",
    "    {'frame': f\"Women who speak out against objectification are audacious and deserve to be mocked.\",\n",
    "     'problem': [\"Derogatory labeling\",\"Invalidating womens experiences\"],\n",
    "     'rationale': [\"The problem of derogatory labeling arises as the meme portrays mocks a womans opinion using derogatory language.\", \"The problem of invalidating womens experiences arises as the meme dismisses womans concerns about objectification, suggesting that their feelings are not valid or worthy of respect.\"]},\n",
    "     ]}},\n",
    "\n",
    "{'filename': '101.jpg', 'result':{'frame_problems':[\n",
    "    {'frame': f\"Women's primary role is to perform domestic tasks.\",\n",
    "     'problem': ['Stereotyping'],\n",
    "     'rationale': [f\"The problem of stereotyping arises as the meme reinforces the stereotype that a woman's primary role is to perform domestic tasks, limiting her identity and capabilities to traditional gender roles.\"],\n",
    "     },\n",
    "     {'frame': f\"Women are like household appliances and should be maintained for their utility.\",\n",
    "      'problem': ['Disposability'],\n",
    "      'rationale': [f\"The problem of disposability arises as the meme equates a woman to a household appliance that must be cleaned and maintained for prolonged use, reducing her to a utilitarian object rather than a person.\"]},\n",
    "]}},\n",
    "{'filename': '10436.jpg', 'result':{'frame_problems':[\n",
    "    {'frame': \"Women are only valuable for performing sexual acts for men.\",\n",
    "    'problem': ['Objectification','Sexual entitlement'],\n",
    "    'rationale': [f\"The problem of objectification arises as the text reduces women to their ability to perform a sexual act, disregarding their individuality and worth as human beings.\", f\"The problem of sexual entitlement arises as the meme conveys that if a woman does not perform oral sex she is worthless, reflecting the belief that men are entitled to women's sexual services.\"]},\n",
    "]}},\n",
    "{'filename': '10376.jpg', 'result':{'frame_problems':[\n",
    "    {'frame': f\"Women deserve to be judged harshly based on their looks, especially body size and are subject to ridicule if they do not meet certain standards of attractiveness.\",\n",
    "    'problem': ['Fat shaming','Appearance-based discrimination'],\n",
    "    'rationale': [f\"The problem of fat shaming arises as the meme mocks the woman's appearance, specifically her body size by using her photo to provoke a 'WTF' reaction.\", f\"The problem of appearance-based discrimination arises as the meme suggests that the person in the image does not meet certain standards of attractiveness or presentation, thereby inviting ridicule or judgment based solely on their appearance.\"]},\n",
    "]}},\n",
    "{'filename': '10032.jpg', 'result':{'frame_problems':[\n",
    "    {'frame':f\"The kitchen is associated with women and female roles.\",\n",
    "    'problem': ['Stereotyping'],\n",
    "    'rationale': [f\"The problem of stereotyping arises as the meme suggests a gendered connection between the kitchen and female children, reinforcing traditional gender roles and stereotypes.\"]},\n",
    "    {'frame':f\"Having sex in a kitchen increases the chances of having a girl child.\",\n",
    "    'problem': ['Pseudoscience'],\n",
    "    'rationale': [f\"The problem of pseudoscience arises as the meme presents a false claim that having sex in the kitchen can influence the child's gender, reflecting the use of misleading facts about biology.\"]},\n",
    "]}},    \n",
    "{'filename': '10032.jpg', 'result':{'frame_problems':[\n",
    "    {'frame': f\"Women in marriage are violent and dangerous.\",\n",
    "    'problem': ['Stereotyping', 'Demonization'],\n",
    "    'rationale': [f\"The problem of stereotyping arises as the the image depicts a woman in a violent and threatening manner, reinforcing harmful stereotypes about women in marriage.\", f\"The problem of demonizing arises as the meme portrays women as inherently dangerous or harmful within the context of marriage.\"]},\n",
    "]}},\n",
    "{'filename': '10032.jpg', 'result':{'frame_problems':[\n",
    "    {'frame': f\"Men have simple and non-violent sexual fantasies unlike women.\",\n",
    "    'problem': ['Stereotyping', f\"Trivializing women's sexual satisfaction\"],\n",
    "    'rationale': [f\"The problem of stereotyping arises as the meme generalizes and misrepresents the sexual fantasies of women.\", f\"The problem of trivializing women's sexual satisfaction arises as the meme mocks and oversimplifies women's sexual desires by reducing their fantasies to wanting to be choked, thereby belittling and dismissing women's sexual needs.\"]},\n",
    "]}},\n",
    "{'filename': '4128.jpg', 'result':{'frame_problems':[\n",
    "    {'frame': f\"Women's value is primarily tied to their sexual services.\",\n",
    "    'problem': ['Objectification'],\n",
    "    'rationale': [f\"The problem of objectification arises as the meme objectifies women, valuing them solely based on their economic cost and sexual services.\"]},\n",
    "    {'frame': f\"A wife is an overpriced service compared to a full-time hooker.\",\n",
    "    'problem':['Transactional relationships'],\n",
    "    'rationale': [f\"The problem of transactional relationships arises as the meme reduces both marriage and sex work to purely financial transactions, suggesting that a wife is an overpriced service compared to a full time hooker.\"]},\n",
    "]}},\n",
    "{'filename': '3898.jpg', 'result':{'frame_problems':[\n",
    "    {'frame': f\"Women are deceitful because they use makeup to alter their appearance.\",\n",
    "    'problem': ['False equivalence'],\n",
    "    'rationale': [f\"The problem of false equivalence arises as the meme equates the use of makeup with deceitfulness by implying that just as men lie verbally, women lie about their appearance, creating a misleading comparison that unfairly stereotypes women as deceptive.\"]},\n",
    "]}},\n",
    "{'filename': '343.jpg', 'result':{'frame_problems':[\n",
    "    {'frame':f\"Physical violence is a valid way to deal with women who don't comply.\",\n",
    "    'problem': ['Violence','Patriarchal control'],\n",
    "    'rationale': [f\"The problem of violence arises as the text implies that women should be controlled through physical punishment.\", f\"The problem of patriarchal control arises as the meme suggests that women should be punished for not complying with men's demands.\"]},\n",
    "]}},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0582cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for demo in demos2:\n",
    "    frames = []\n",
    "    frationales = []\n",
    "    problems = []\n",
    "    prationales = []\n",
    "    \n",
    "    for line in read_jsonl('total.jsonl'):\n",
    "        if line['filename'] == demo['filename']:\n",
    "            # Fix the content key if it's a string\n",
    "            if isinstance(line['content'], str):\n",
    "                try:\n",
    "                    line['content'] = json.loads(line['content'])\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Could not parse content for {line['filename']}\")\n",
    "                    continue\n",
    "            \n",
    "            for frame in line['articulations']:\n",
    "                frames.append(frame['text'])\n",
    "                frationales.append(frame['reasoning'])\n",
    "            \n",
    "            for problem in line['content']['identified_problems']:\n",
    "                clean_problem = re.sub(r'^\\d+(\\.\\d+)*\\s+', '', problem['problem'])\n",
    "                problems.append(clean_problem)\n",
    "                prationales.append(problem['rationale'])\n",
    "            \n",
    "            demo['frames'] = frames\n",
    "            demo['frationales'] = frationales\n",
    "            demo['problems'] = problems\n",
    "            demo['prationales'] = prationales\n",
    "            break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8476c9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_user_prompt(user_prompt: str, problems: list[str], rationales: list[str], frames: list[str] | None = None, frame_rationale: list[str] | None = None, demo: bool = False, demo_result = None) -> list[dict]:\n",
    "\n",
    "    formatted_frames = \"\\n\".join(frames) if frames else \"\"\n",
    "    formatted_problems = \"\\n\".join(problems) if problems else \"\"\n",
    "    formatted_rationales = \"\\n\".join(rationales) if rationales else \"\"\n",
    "    frame_rationales = \"\\n\".join(frame_rationale) if frame_rationale else \"\"\n",
    "    \n",
    "    formatted_prompt = user_prompt.format(\n",
    "        frames=formatted_frames,\n",
    "        frationales= frame_rationales,\n",
    "        problems=formatted_problems,\n",
    "        prationales=formatted_rationales\n",
    "    )\n",
    "    \n",
    "    msg = []\n",
    "    msg.append(\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": formatted_prompt},\n",
    "\n",
    "        ]}\n",
    "    )\n",
    "    if demo:   \n",
    "        msg.append(\n",
    "            {\"role\": \"assistant\", \"content\": json.dumps(demo_result)}\n",
    "        )\n",
    "    return msg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728224af",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file_path = 'prompts/problem_to_frame 2.yaml'\n",
    "with open(config_file_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    config = ChatCompletionConfig(**config)\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'], timeout=90)\n",
    "sys_prompt = config.system_prompt.strip()\n",
    "user_prompt= config.user_prompt.strip()\n",
    "processed=set()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3accd6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg=[{\"role\": \"system\", \"content\": sys_prompt}]\n",
    "for demo in demos2:\n",
    "    msg.extend(make_user_prompt(user_prompt, demo['problems'], demo['prationales'], demo['frames'],demo['frationales'], True, demo['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd66faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "SAVE_INTERVAL = 500\n",
    "counter = 0\n",
    "\n",
    "def append_jsonl(filepath, data):\n",
    "    with open(filepath, 'a', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "\n",
    "def save_processed_set(filepath, processed_set):\n",
    "    with open(filepath, 'wb') as f:\n",
    "        pickle.dump(processed_set, f)\n",
    "\n",
    "# Initialize\n",
    "all_results = []\n",
    "\n",
    "for line in tqdm(read_jsonl('total.jsonl')):\n",
    "    filename = line.get('filename')\n",
    "    if (filename in no_total) or (filename in processed):\n",
    "        continue\n",
    "\n",
    "    frames = []\n",
    "    frationales = []\n",
    "    problems = []\n",
    "    prationales = []\n",
    "\n",
    "    for frame in line.get('articulations', []):\n",
    "        frames.append(frame.get('text', ''))\n",
    "        frationales.append(frame.get('reasoning', ''))\n",
    "\n",
    "    raw_content = line.get('content')\n",
    "    if raw_content is None:\n",
    "        print(f\"Warning: No content for {filename}\")\n",
    "        continue\n",
    "\n",
    "    if isinstance(raw_content, str):\n",
    "        try:\n",
    "            raw_content = json.loads(raw_content)\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Warning: Could not parse content for {filename}\")\n",
    "            continue\n",
    "\n",
    "    for problem in raw_content.get('identified_problems', []):\n",
    "        clean_problem = re.sub(r'^\\d+(\\.\\d+)*\\s+', '', problem.get('problem', ''))\n",
    "        problems.append(clean_problem)\n",
    "        prationales.append(problem.get('rationale', ''))\n",
    "\n",
    "    msg.extend(make_user_prompt(user_prompt, problems, prationales, frames, frationales))\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=config.model,\n",
    "            messages=msg,\n",
    "            temperature=config.temperature,\n",
    "            seed=config.seed,\n",
    "            response_format=config.response_format,\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"API call failed for {filename}: {e}\")\n",
    "        msg.pop()\n",
    "        continue\n",
    "\n",
    "    msg.pop()\n",
    "\n",
    "    if not completion:\n",
    "        print(f\"Skipping meme {filename} due to API safety error\")\n",
    "    else:\n",
    "        content = completion.choices[0].message.content\n",
    "        print(f\"{filename} {completion.usage.prompt_tokens_details.cached_tokens}\")\n",
    "        result = {\"filename\": filename, \"content\": content}\n",
    "        all_results.append(result)\n",
    "        processed.add(filename)\n",
    "        counter += 1\n",
    "\n",
    "    # Save every 500 memes\n",
    "    if counter % SAVE_INTERVAL == 0:\n",
    "        print(f\"Saving {len(all_results)} results and processed set...\")\n",
    "        append_jsonl(\"mappings.jsonl\", all_results)\n",
    "        save_processed_set(\"processed.pkl\", processed)\n",
    "        all_results = []\n",
    "\n",
    "# Final save for remaining data\n",
    "if all_results:\n",
    "    print(f\"Saving final {len(all_results)} results and processed set...\")\n",
    "    append_jsonl(\"mappings.jsonl\", all_results)\n",
    "    save_processed_set(\"processed.pkl\", processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7938eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_processed_set(processed_set: set, file_path: str):\n",
    "    \"\"\"Save a set of processed filenames to a file.\"\"\"\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(processed_set, f)\n",
    "\n",
    "def load_processed_set(file_path: str) -> set:\n",
    "    \"\"\"Load a set of processed filenames from a file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return set()\n",
    "save_processed_set(processed, 'processed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Before making the API call, debug your messages\n",
    "try:\n",
    "    print(\"Messages structure:\")\n",
    "    for i, msg in enumerate(msg):\n",
    "        print(f\"Message {i}: {msg['role']}\")\n",
    "        if isinstance(msg['content'], list):\n",
    "            print(f\"  Content type: list with {len(msg['content'])} items\")\n",
    "            for j, item in enumerate(msg['content']):\n",
    "                print(f\"    Item {j}: {type(item)} - {item.get('type', 'no type')}\")\n",
    "        else:\n",
    "            print(f\"  Content type: {type(msg['content'])}\")\n",
    "    \n",
    "    # Test JSON serialization\n",
    "    json.dumps(msg)\n",
    "    print(\"Messages are JSON serializable\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in messages structure: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9439ff37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': {'frame_problems': [{'frame': 'Women who speak out against objectification '\n",
      "                                          'are audacious and deserve to be mocked.',\n",
      "                                 'problem': ['Derogatory labeling',\n",
      "                                             \"Invalidating women's experiences\"],\n",
      "                                 'rationale': ['The problem of derogatory labeling '\n",
      "                                               'arises as the meme portrays mocks a '\n",
      "                                               \"woman's opinion using derogatory \"\n",
      "                                               'language.',\n",
      "                                               \"The problem of invalidating women's \"\n",
      "                                               'experiences arises as the meme dismisses '\n",
      "                                               \"woman's concerns about objectification, \"\n",
      "                                               'suggesting that their feelings are not '\n",
      "                                               'valid or worthy of respect.']}]},\n",
      " 'role': 'assistant'}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(msg[2],width=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4274f309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 8405\n",
      "Total problems: 118\n",
      "Trivializing consent: 160\n",
      "Reductionism: 645\n",
      "Disposability: 200\n",
      "Sexual innuendo: 788\n",
      "Sexualization: 541\n",
      "Intellectual degradation: 74\n",
      "Stereotyping: 411\n",
      "Derogatory labeling: 634\n",
      "Double standards: 160\n",
      "Gender essentialism: 373\n",
      "Unrealistic beauty standards: 286\n",
      "Objectification: 1089\n",
      "Misunderstanding feminism: 351\n",
      "Minimizing feminist efforts: 363\n",
      "Undermining women’s rights movements: 22\n",
      "Invalidating women’s experiences: 126\n",
      "Appearance-based discrimination: 285\n",
      "Ownership: 61\n",
      "Ageism: 56\n",
      "Promoting rape culture: 269\n",
      "Violence: 247\n",
      "Invalidating women's experiences: 103\n",
      "Demonization: 103\n",
      "Dismissiveness: 70\n",
      "Disrespecting sex workers: 110\n",
      "Ridicule: 942\n",
      "Scapegoating: 59\n",
      "Enforced gender norms: 177\n",
      "Fearmongering: 10\n",
      "Neo sexism: 9\n",
      "Coercion: 236\n",
      "Women subjugation: 7\n",
      "Exclusion: 29\n",
      "Reverse sexism: 22\n",
      "Incest: 15\n",
      "Transactional relationships: 133\n",
      "Sexual entitlement: 216\n",
      "Promoting infidelity: 29\n",
      "Trivializing infidelity: 13\n",
      "Victim blaming: 110\n",
      "Undermining women’s capabilities: 59\n",
      "Conditional respect: 25\n",
      "Trivializing sexual assault: 82\n",
      "False accusations: 41\n",
      "Trivializing women’s issues: 57\n",
      "Trivializing serious issues: 11\n",
      "Women Valuation: 20\n",
      "Trivializing mental health issues: 15\n",
      "Undermining women's capabilities: 43\n",
      "Menstruation stigma: 45\n",
      "Age-related sexualization: 109\n",
      "Mistrust: 77\n",
      "Stigmatization: 26\n",
      "Trivializing women's issues: 68\n",
      "Dehumanization of women: 180\n",
      "Misrepresentation: 53\n",
      "Male validation: 7\n",
      "Possessiveness: 18\n",
      "False equivalence: 35\n",
      "Dismissing women's rights: 18\n",
      "Fat shaming: 344\n",
      "Workplace discrimination: 10\n",
      "Divisiveness: 24\n",
      "Policing women’s bodies: 14\n",
      "Sexual harassment: 31\n",
      "Gatekeeping: 11\n",
      "Undermining women's rights movements: 39\n",
      "Biased judgement: 3\n",
      "Patriarchal attitudes: 2\n",
      "Demeaning aspirations: 23\n",
      "Trivializing oppression: 15\n",
      "Trivializing prostitution: 17\n",
      "Deflection: 24\n",
      "Racism: 21\n",
      "Pedophilia exploitation: 23\n",
      "Patriarchal control: 53\n",
      "Ableism: 27\n",
      "Classism: 10\n",
      "Intersectional prejudice: 30\n",
      "Biased Judgement: 21\n",
      "Forced marriage: 2\n",
      "Lack of accountability by men: 2\n",
      "Commitment phobia: 12\n",
      "Single motherhood stigmatization: 9\n",
      "Transphobia: 42\n",
      "Trivializing women's sexual satisfaction: 16\n",
      "Policing women's bodies: 14\n",
      "Wage disparity: 6\n",
      "Endorsing necrophilia: 7\n",
      "Trivializing women’s sexual satisfaction: 8\n",
      "Trivializing addiction: 6\n",
      "Pseudoscience: 4\n",
      "Cultural insensitivity: 17\n",
      "Dismissing women’s rights: 14\n",
      "Women Subjugation: 14\n",
      "Endorsing marital rape: 4\n",
      "Trivializing eating disorders: 3\n",
      "Dehumanization: 7\n",
      "Promoting self-harm: 7\n",
      "Homophobia: 5\n",
      "Biased judgment: 2\n",
      "Trivializing the need for representation: 5\n",
      "Trivializing women’s consent: 1\n",
      "Toxic masculinity: 6\n",
      "Professional misconduct: 1\n",
      "Conspiracy thinking: 6\n",
      "Exploitation of Women by Men: 1\n",
      "Trivializing feminist efforts: 1\n",
      "Disrespect towards women: 2\n",
      "Disrespecting womens dignity: 1\n",
      "Dismissiveness (Invalidating women’s experiences): 1\n",
      "Invalidating womens experiences: 1\n",
      "Trivializing women\u0019s issues: 1\n",
      "6.5.1 Gender essentialism: 1\n",
      "7.6.1 Invalidating women’s experiences: 2\n",
      "6.4.4 Coercion: 1\n",
      "8.1 Disposability: 1\n",
      "4.2 Ridicule: 1\n"
     ]
    }
   ],
   "source": [
    "#analyzing distribution of frames per problem\n",
    "\n",
    "def count_frames_and_problems(jsonl_path):\n",
    "    problem_frame_count = defaultdict(int)\n",
    "    total_frames = 0\n",
    "\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            content_str = data.get('content')\n",
    "            if not content_str:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                content = json.loads(content_str)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping malformed content in file {data.get('filename')}\")\n",
    "                continue\n",
    "\n",
    "            for frame_entry in content.get('frame_problems', []):\n",
    "                total_frames += 1\n",
    "                problems = frame_entry.get('problem', [])\n",
    "                for prob in problems:\n",
    "                    problem_frame_count[prob] += 1\n",
    "\n",
    "    return total_frames, dict(problem_frame_count)\n",
    "\n",
    "total_frames, counts = count_frames_and_problems(\"mappings.jsonl\")\n",
    "print(f\"Total frames: {total_frames}\")\n",
    "print(f\"Total problems: {len(counts)}\")\n",
    "for problem, count in counts.items():\n",
    "    print(f\"{problem}: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b301e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map specific problem replacements\n",
    "manual_replacements = {\n",
    "    \"dehumanization\": \"dehumanization of women\",\n",
    "    \"mistrust\": \"mistrust in women\",\n",
    "    \"pseudoscience\": \"sexist pseudoscience\",\n",
    "    \"invalidating womens experiences\": \"invalidating women's experiences\",\n",
    "    \"dismissiveness (invalidating women's experiences)\": \"dismissiveness\",\n",
    "    \"trivializing women's consent\": \"trivializing consent\",\n",
    "    \"biased judgment\" : \"biased judgement\",\n",
    "    \"trivializing feminist efforts\": \"minimizing feminist efforts\",\n",
    "    \"disrespecting womens dignity\": \"disrespect towards women\"\n",
    "    \n",
    "}\n",
    "\n",
    "def normalize_problem(p):\n",
    "    # Fix weird characters\n",
    "    p = p.replace(\"’\", \"'\").replace(\"\u0019\", \"'\")\n",
    "    # Remove leading numbers like \"6.5.1 \" or \"2 \"\n",
    "    p = re.sub(r'^\\d+(\\.\\d+)*\\s+', '', p)\n",
    "    # Normalize spacing and case\n",
    "    p = p.strip().lower()\n",
    "    # Apply manual mappings after normalization\n",
    "    return manual_replacements.get(p, p)\n",
    "\n",
    "def clean_mappings(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            entry = json.loads(line)\n",
    "            content_str = entry.get(\"content\", \"{}\")\n",
    "\n",
    "            try:\n",
    "                content = json.loads(content_str)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping bad content in {entry.get('filename')}\")\n",
    "                continue\n",
    "\n",
    "            for fp in content.get(\"frame_problems\", []):\n",
    "                cleaned_problems = list({normalize_problem(p) for p in fp.get(\"problem\", [])})\n",
    "                fp[\"problem\"] = cleaned_problems\n",
    "\n",
    "            entry[\"content\"] = json.dumps(content, ensure_ascii=False)\n",
    "            outfile.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "# Run it\n",
    "clean_mappings(\"mappings.jsonl\", \"mappings_cleaned.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3cd8c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames: 8405\n",
      "Total problems: 96\n",
      "trivializing consent: 161\n",
      "reductionism: 645\n",
      "sexual innuendo: 788\n",
      "disposability: 201\n",
      "sexualization: 541\n",
      "intellectual degradation: 74\n",
      "stereotyping: 411\n",
      "derogatory labeling: 634\n",
      "double standards: 160\n",
      "gender essentialism: 374\n",
      "unrealistic beauty standards: 286\n",
      "objectification: 1089\n",
      "minimizing feminist efforts: 364\n",
      "misunderstanding feminism: 351\n",
      "undermining women's rights movements: 61\n",
      "invalidating women's experiences: 232\n",
      "appearance-based discrimination: 285\n",
      "ownership: 61\n",
      "ageism: 56\n",
      "promoting rape culture: 269\n",
      "violence: 247\n",
      "demonization: 103\n",
      "dismissiveness: 71\n",
      "disrespecting sex workers: 110\n",
      "ridicule: 943\n",
      "scapegoating: 59\n",
      "enforced gender norms: 177\n",
      "fearmongering: 10\n",
      "neo sexism: 9\n",
      "coercion: 237\n",
      "women subjugation: 21\n",
      "exclusion: 29\n",
      "reverse sexism: 22\n",
      "incest: 15\n",
      "transactional relationships: 133\n",
      "sexual entitlement: 216\n",
      "trivializing infidelity: 13\n",
      "promoting infidelity: 29\n",
      "victim blaming: 110\n",
      "undermining women's capabilities: 102\n",
      "conditional respect: 25\n",
      "trivializing sexual assault: 82\n",
      "false accusations: 41\n",
      "trivializing women's issues: 126\n",
      "trivializing serious issues: 11\n",
      "women valuation: 20\n",
      "trivializing mental health issues: 15\n",
      "menstruation stigma: 45\n",
      "age-related sexualization: 109\n",
      "mistrust in women: 77\n",
      "stigmatization: 26\n",
      "dehumanization of women: 187\n",
      "misrepresentation: 53\n",
      "male validation: 7\n",
      "possessiveness: 18\n",
      "false equivalence: 35\n",
      "dismissing women's rights: 32\n",
      "fat shaming: 344\n",
      "workplace discrimination: 10\n",
      "divisiveness: 24\n",
      "policing women's bodies: 28\n",
      "sexual harassment: 31\n",
      "gatekeeping: 11\n",
      "biased judgement: 26\n",
      "patriarchal attitudes: 2\n",
      "demeaning aspirations: 23\n",
      "trivializing oppression: 15\n",
      "trivializing prostitution: 17\n",
      "deflection: 24\n",
      "racism: 21\n",
      "pedophilia exploitation: 23\n",
      "patriarchal control: 53\n",
      "ableism: 27\n",
      "classism: 10\n",
      "intersectional prejudice: 30\n",
      "forced marriage: 2\n",
      "lack of accountability by men: 2\n",
      "commitment phobia: 12\n",
      "single motherhood stigmatization: 9\n",
      "transphobia: 42\n",
      "trivializing women's sexual satisfaction: 24\n",
      "wage disparity: 6\n",
      "endorsing necrophilia: 7\n",
      "trivializing addiction: 6\n",
      "sexist pseudoscience: 4\n",
      "cultural insensitivity: 17\n",
      "endorsing marital rape: 4\n",
      "trivializing eating disorders: 3\n",
      "promoting self-harm: 7\n",
      "homophobia: 5\n",
      "trivializing the need for representation: 5\n",
      "toxic masculinity: 6\n",
      "professional misconduct: 1\n",
      "conspiracy thinking: 6\n",
      "exploitation of women by men: 1\n",
      "disrespect towards women: 3\n"
     ]
    }
   ],
   "source": [
    "total_frames, counts = count_frames_and_problems(\"mappings_cleaned.jsonl\")\n",
    "print(f\"Total frames: {total_frames}\")\n",
    "print(f\"Total problems: {len(counts)}\")\n",
    "for problem, count in counts.items():\n",
    "    print(f\"{problem}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c42ea7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processed 8004 unique frames into 96 problem files at 'problem_frames'.\n"
     ]
    }
   ],
   "source": [
    "def process_mappings_jsonl(input_path='mappings_cleaned.jsonl', output_dir='problem_frames'):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    frame_id_map = {}\n",
    "    problem_to_frames = defaultdict(list)\n",
    "    frame_counter = 1\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            filename = data['filename']\n",
    "            content = json.loads(data['content'])\n",
    "            \n",
    "            for item in content.get('frame_problems', []):\n",
    "                frame_text = item['frame'].strip()\n",
    "                rationale = item.get('rationale', [])\n",
    "                problems = [p for p in item.get('problem', [])]\n",
    "\n",
    "                # Assign unique frame ID\n",
    "                if frame_text not in frame_id_map:\n",
    "                    frame_id_map[frame_text] = f\"f{frame_counter}\"\n",
    "                    frame_counter += 1\n",
    "\n",
    "                frame_entry = {\n",
    "                    \"id\": frame_id_map[frame_text],\n",
    "                    \"frame\": frame_text,\n",
    "                    \"rationale\": rationale,\n",
    "                    \"source\": filename\n",
    "                }\n",
    "\n",
    "                # Assign to each normalized problem\n",
    "                for problem in problems:\n",
    "                    problem_to_frames[problem].append(frame_entry)\n",
    "\n",
    "    # Write one file per unique problem\n",
    "    for problem, frames in problem_to_frames.items():\n",
    "        # Sanitize filename\n",
    "        problem_filename = re.sub(r'[^a-zA-Z0-9_]', '_', problem)\n",
    "        output_path = os.path.join(output_dir, f\"{problem_filename}.jsonl\")\n",
    "\n",
    "        with open(output_path, 'w', encoding='utf-8') as out_file:\n",
    "            for frame_entry in frames:\n",
    "                out_file.write(json.dumps(frame_entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    print(f\"✅ Processed {len(frame_id_map)} unique frames into {len(problem_to_frames)} problem files at '{output_dir}'.\")\n",
    "\n",
    "process_mappings_jsonl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c93504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "def",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
